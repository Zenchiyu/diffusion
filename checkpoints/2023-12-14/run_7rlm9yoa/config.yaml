defaults:
  - _self_
  # Add one of the following right after to use a pre-trained model
  # - /cifar10/cond@_here_
  # - /cifar10/cond_ablation_attention@_here_
  # - /cifar10/uncond@_here_

  # - /fashionmnist/cond@_here_
  # - /fashionmnist/cond_ablation_attention@_here_
  # - /fashionmnist/uncond@_here_

  # - /celeba/uncond_big@_here_
  # - /celeba/uncond_small@_here_
  # - /celeba/uncond_tiny@_here_

wandb:
  project: diffusion
  mode: online

wandb_watch: True  # log more info

common:
  nb_epochs: 100  # people usually track nÂ° of gradient steps instead
  sampling:
    method: stochastic_heun  # stochastic (euler), stochastic_heun, null (euler), heun
    num_steps: 50
    label: null     # class label for Classifier-Free Guidance (CFG)
    cfg_scale: 0    # = 0 for uncond., > 0 for CFG, in practice people use > 1
    save_path: ./results/images/samples_celeba_uncond_tiny.png  # relative to cwd
  training:
    p_uncond: 0.2  # used to train the uncond. and cond. score function
    chkpt_path: ./checkpoints/checkpoint_celeba_uncond_tiny.pth  # relative to cwd
  # CFG is ignored when dataset has no labels
  # and the model is only trained uncond. (but still conditioned on noise)

diffusion:
  sigma_min: 2e-3
  sigma_max: 80

optim:  # Adam optimizer
  lr: 1e-4

# I usually change these below:
dataset:
  name: CelebA  # FashionMNIST, CIFAR10, CelebA
  root_dir: ./data  # relative to cwd
  batch_size: 64
  num_workers: 4

model:
  min_channels: 64
  depths: [1, 2, 4]
  # resolutions: 128 -> 64 -> 32. 16 in the bridge.
  # channels: 64 -> 128 -> 256. 512 in the bridge.
  # bridge has the same 'depth' as depths[-1].
  # first resolution level, no downsampling.
  cond_channels: 8
  self_attentions: [False, False, False]
  self_attention_bridge: True
  nb_heads: [0, 0, 4]  # twice the num. of heads than before 
